import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from src.utils import apply_science_style
apply_science_style()
import seaborn as sns
import os
import logging
import argparse
from pathlib import Path
import sys
from tqdm import tqdm

# Add the project root directory to the Python path
project_root = str(Path(__file__).resolve().parent.parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)

# Import utility functions - assuming they are in src/utils.py
try:
    from src.utils import unscale_feature, transform_circular_features
except ImportError:
    logging.error("Could not import utility functions from src.utils. Ensure the file exists and sys.path is correct.")
    sys.exit(1)

# --- Configuration ---

# Set up logging
os.makedirs('logs', exist_ok=True)
log_file = 'logs/feature_distribution_analysis.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Constants from training script (or potentially a shared constants file)
DATASET_PATH_DEFAULT = 'results/datasets/training_datasets_pixels.parquet'
PHENOLOGY_MAPPING = {1: 'Deciduous', 2: 'Evergreen'}
PHENOLOGY_COLORS = {1: 'darkorange', 2: 'royalblue'} # New colors
INDICES = ['ndvi', 'evi', 'nbr', 'crswir'] # Base indices for transformations
FEATURE_TYPES_TO_UNSCALE = {
    'amplitude_h1': 'amplitude',
    'amplitude_h2': 'amplitude',
    'phase_h1': 'phase',
    'phase_h2': 'phase',
    'offset': 'offset',
    'var_residual': 'variance'
}
# Features generated by circular transformation
CIRCULAR_FEATURE_SUFFIXES = ['_cos', '_sin']

# --- Main Function ---

def main():
    parser = argparse.ArgumentParser(description='Analyze feature distributions by eco-region and phenology.')
    parser.add_argument('--dataset_path', type=str, default=DATASET_PATH_DEFAULT,
                        help=f'Path to the dataset parquet file (default: {DATASET_PATH_DEFAULT}).')
    parser.add_argument('--output_dir', '-o', type=str, default='results/analysis/feature_distributions_by_eco_region',
                        help='Directory to save the distribution plots.')
    # parser.add_argument('--features', '-f', type=str, default=None,
    #                     help='Optional: Comma-separated list of specific features to analyze. Default is all harmonic features.')
    parser.add_argument('--test', '-t', action='store_true',
                        help='Run in test mode with a small subset of data.')
    parser.add_argument('--test_size', type=int, default=5000,
                        help='Number of samples to use in test mode (default: 5000).')
    parser.add_argument('--col_wrap', type=int, default=4,
                        help='Number of columns for facet grid wrapping (default: 4).')


    args = parser.parse_args()

    # --- Force col_wrap=1 for one eco-region per row ---
    args.col_wrap = 1
    logger.info("Setting col_wrap=1 to display one eco-region per row.")

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    logger.info(f"Output directory: {args.output_dir}")
    logger.info(f"Log file: {log_file}")
    if args.test:
        logger.info(f"Running in TEST MODE with {args.test_size} samples")

    # --- Load Data ---
    logger.info(f"Loading dataset from {args.dataset_path}...")
    try:
        df = pd.read_parquet(args.dataset_path)
        # Ensure necessary columns exist
        required_cols = ['eco_region', 'phenology']
        if not all(col in df.columns for col in required_cols):
            missing = [col for col in required_cols if col not in df.columns]
            logger.error(f"Dataset missing required columns: {missing}")
            return
        logger.info(f"Dataset loaded: {len(df)} samples, {len(df.columns)} columns")
    except Exception as e:
        logger.error(f"Failed to load dataset from {args.dataset_path}: {e}")
        return

    # --- Test Mode Sampling ---
    if args.test:
        if len(df) > args.test_size:
            logger.info(f"Sampling {args.test_size} records for test mode...")
            # Try stratified sampling by eco_region and phenology if possible
            stratify_cols = ['eco_region', 'phenology']
            if all(col in df.columns for col in stratify_cols) and df[stratify_cols].nunique().prod() > 1:
                 try:
                     # Sample a fraction, aiming for test_size, grouping by strata
                     frac = args.test_size / len(df)
                     df = df.groupby(stratify_cols, group_keys=False).apply(lambda x: x.sample(frac=frac, random_state=42) if len(x) > 1 else x)
                     # If sampling resulted in too few samples due to small groups, top up randomly
                     if len(df) < args.test_size * 0.8:
                         logger.warning("Stratified sampling resulted in fewer samples than desired, potentially due to small strata. Falling back to random sampling.")
                         df = pd.read_parquet(args.dataset_path).sample(args.test_size, random_state=42)
                 except Exception as e:
                     logger.warning(f"Stratified sampling failed ({e}), falling back to random sampling.")
                     df = df.sample(args.test_size, random_state=42)

            else:
                 df = df.sample(args.test_size, random_state=42)
            logger.info(f"Using subset of data: {len(df)} samples")
        else:
            logger.warning(f"Test size ({args.test_size}) is larger than dataset size ({len(df)}). Using full dataset.")

    # --- Preprocess Data ---
    logger.info("Preprocessing features: Unscaling and applying circular transformations...")
    df_processed = df.copy()

    # 1. Unscale features
    unscaled_count = 0
    skipped_unscale = []
    with tqdm(total=len(INDICES) * len(FEATURE_TYPES_TO_UNSCALE), desc="Unscaling Features") as pbar:
        for index in INDICES:
            for ftype_suffix, feature_type in FEATURE_TYPES_TO_UNSCALE.items():
                col_name = f"{index}_{ftype_suffix}"
                if col_name in df_processed.columns:
                    try:
                        df_processed[col_name] = unscale_feature(
                            df_processed[col_name],
                            feature_type=feature_type,
                            index_name=index
                        )
                        unscaled_count += 1
                    except Exception as e:
                        logger.warning(f"Could not unscale {col_name}: {e}")
                        skipped_unscale.append(col_name)
                else:
                    skipped_unscale.append(col_name) # Column not found
                pbar.update(1)
    logger.info(f"Unscaling complete. Processed {unscaled_count} columns.")
    if skipped_unscale:
        # Log only unique missing/skipped columns concisely
        unique_skipped = sorted(list(set(col for col in skipped_unscale if col not in df_processed.columns)))
        if unique_skipped:
            logger.debug(f"Columns not found for unscaling: {unique_skipped}")


    # 2. Apply circular transformation
    try:
        df_processed = transform_circular_features(df_processed, INDICES)
        logger.info("Circular transformation applied.")
    except Exception as e:
        logger.error(f"Error during circular transformation: {e}")
        # Decide whether to proceed without transformation or stop
        logger.warning("Proceeding without circular features potentially missing.")
        # return # Or continue carefully

    # --- Identify Features to Plot ---
    features_to_plot = []
    potential_suffixes = list(FEATURE_TYPES_TO_UNSCALE.keys()) \
                       + [f"phase_h{i+1}{s}" for i in range(2) for s in CIRCULAR_FEATURE_SUFFIXES]

    # Remove original phase features if transformed versions exist
    if any('_phase_h1_cos' in col or '_phase_h1_sin' in col for col in df_processed.columns):
        if 'phase_h1' in potential_suffixes: potential_suffixes.remove('phase_h1')
    if any('_phase_h2_cos' in col or '_phase_h2_sin' in col for col in df_processed.columns):
        if 'phase_h2' in potential_suffixes: potential_suffixes.remove('phase_h2')

    for index in INDICES:
        for suffix in potential_suffixes:
            feature_name = f"{index}_{suffix}"
            if feature_name in df_processed.columns:
                features_to_plot.append(feature_name)

    # # Optional: Filter based on user input
    # if args.features:
    #     user_features = [f.strip() for f in args.features.split(',')]
    #     # Validate user features
    #     valid_user_features = [f for f in user_features if f in df_processed.columns]
    #     missing_user_features = [f for f in user_features if f not in df_processed.columns]
    #     if missing_user_features:
    #         logger.warning(f"Requested features not found after preprocessing: {missing_user_features}")
    #     if not valid_user_features:
    #         logger.error("No valid features specified by user found in the data. Exiting.")
    #         return
    #     features_to_plot = valid_user_features
    #     logger.info(f"Using user-specified features: {features_to_plot}")
    # else:
    #     logger.info(f"Analyzing automatically identified harmonic features ({len(features_to_plot)}).")

    if not features_to_plot:
        logger.error("No features identified for plotting after preprocessing. Check data and feature naming conventions.")
        return

    logger.info(f"Identified {len(features_to_plot)} features for plotting: {features_to_plot}")


    # --- Generate Plots ---
    logger.info("Generating distribution plots...")
    num_eco_regions = df_processed['eco_region'].nunique()
    logger.info(f"Dataset contains {num_eco_regions} unique eco-regions.")

    # Sort eco-regions alphabetically for consistent plot order
    eco_regions_sorted = sorted(df_processed['eco_region'].unique())

    plot_count = 0
    error_count = 0
    for feature in tqdm(features_to_plot, desc="Generating Plots"):
        logger.debug(f"Plotting feature: {feature}")
        try:
            # Create FacetGrid
            g = sns.FacetGrid(
                df_processed,
                col='eco_region',
                col_order=eco_regions_sorted, # Ensure consistent order
                hue='phenology',
                hue_order=[1, 2], # Ensure Deciduous (1), Evergreen (2) order
                palette=PHENOLOGY_COLORS,
                col_wrap=args.col_wrap, # Use the modified col_wrap
                sharex=False, # Allow different x-axis ranges per eco-region
                sharey=False, # Allow different y-axis density scales
                height=2.5, # Adjust height for potentially long vertical plot
                aspect=4 # Adjust aspect ratio for potentially long vertical plot
            )

            # Map the KDE plot
            # common_norm=False ensures KDE is normalized within each subgroup within the facet
            g.map(sns.kdeplot, feature, fill=True, common_norm=False, alpha=0.6, lw=1.5)

            # Add legend and titles
            g.add_legend(title='Phenology', labels=[PHENOLOGY_MAPPING[1], PHENOLOGY_MAPPING[2]])
            g.set_titles(col_template="{col_name}") # Use eco-region name as title for each subplot
            g.set_axis_labels(x_var=f"{feature}", y_var='Density') # Set axis labels clearly

            # Add overall figure title
            g.fig.suptitle(f'Distribution of {feature}\nby Eco-Region and Phenology Class', y=1.03) # Adjust y for spacing

            # Adjust layout
            plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust rect to prevent title overlap

            # Save the figure
            plot_filename = os.path.join(args.output_dir, f'{feature}_distribution.png')
            plt.savefig(plot_filename, bbox_inches='tight', dpi=150) # Use bbox_inches='tight'
            plt.close(g.fig) # Close the figure to free memory
            logger.debug(f"Saved plot: {plot_filename}")
            plot_count += 1

        except Exception as e:
            logger.error(f"Failed to generate plot for feature '{feature}': {e}", exc_info=True)
            error_count += 1
            # Ensure plot is closed if error occurred mid-creation
            plt.close('all')


    logger.info(f"Plot generation complete. Successfully generated {plot_count} plots.")
    if error_count > 0:
        logger.warning(f"{error_count} plots failed to generate.")
    logger.info(f"Plots saved in: {args.output_dir}")

if __name__ == "__main__":
    main() 